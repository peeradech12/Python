{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"สำเนาของ NLTK_Preprocessing.ipynb","provenance":[{"file_id":"1hIe1_rAbNqEs8KiJF7JzbRP53lL_mWPK","timestamp":1634616769555}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CS3hWIuJEe9-"},"source":["## Introduction to Natural Language Processing\n","\n","In this workbook, at a high-level we will learn about text tokenization; text normalization such as lowercasing, stemming; part-of-speech tagging; Named entity recognition; Sentiment analysis; Topic modeling; Word embeddings\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JO1TphwHi65l"},"source":["# Text-PreProcessing\n","The Basics of NLP for Text\n","In this article, we’ll cover the following topics to text-preprocessing:\n","\n","1. Sentence Tokenization\n","2. Word Tokenization\n","3. Regular expression\n","4. Text Lemmatization and Stemming\n","5. Ngram\n","6. Stop Words\n","\n","\n","-----\n"]},{"cell_type":"markdown","metadata":{"id":"jfysmkDyh-BY"},"source":["**punkt ** This tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. It must be trained on a large collection of plaintext in the target language before it can be used.\n","\n","**averaged_perceptron_tagger ** contains the pre-trained English [Part-of-Speech (POS]]\n","\n","**โมดูล nltk (Natural Language Toolkit)** เป็นโมดูลในภาษาไพทอนที่ช่วยในการประมวลภาษาธรรมชาติและโมดูลนี้เป็นที่นิยมกันในโลกนักพัฒนาภาษาไพทอน โดยใช้ Apache License, Version 2.0 และรองรับทั้ง Python 2 และ Python 3."]},{"cell_type":"code","metadata":{"id":"TkXwl0p1G6Gl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634617049446,"user_tz":-420,"elapsed":3932,"user":{"displayName":"พีรเดช สาตสําอางค์","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjerzRPWnypjYRSvGre-9VnuCETGoQsAxXwMDSZA=s64","userId":"05999278997587725662"}},"outputId":"71a518de-490a-443d-c3ff-90a6994b376b"},"source":["import nltk\n","nltk.download('punkt') \n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"L_pNvP0lDAZR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634617049447,"user_tz":-420,"elapsed":9,"user":{"displayName":"พีรเดช สาตสําอางค์","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjerzRPWnypjYRSvGre-9VnuCETGoQsAxXwMDSZA=s64","userId":"05999278997587725662"}},"outputId":"e9550cf1-99c1-4300-c070-6384ebce3110"},"source":["#Tokenization -- Paragraphs into sentences;\n","from nltk.tokenize import sent_tokenize \n","  \n","text = \"Hello All. Welcome to medium. This article is about NLP using NLTK.\"\n","print(\"SENTENCE AS TOKENS:\")\n","print(sent_tokenize(text))\n","print(\"No of Sentence Tokens:\",len(sent_tokenize(text)))"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["SENTENCE AS TOKENS:\n","['Hello All.', 'Welcome to medium.', 'This article is about NLP using NLTK.']\n","No of Sentence Tokens: 3\n"]}]},{"cell_type":"code","metadata":{"id":"pqkXKzNOG_CP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634617064734,"user_tz":-420,"elapsed":313,"user":{"displayName":"พีรเดช สาตสําอางค์","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjerzRPWnypjYRSvGre-9VnuCETGoQsAxXwMDSZA=s64","userId":"05999278997587725662"}},"outputId":"54541413-1f62-4dfd-afdf-e31a5ba9fbe9"},"source":["import nltk.data \n","etext = 'Punkt knows that the periods in Mr. Smith and Johann S. Bach do not mark sentence boundaries.  And sometimes sentences can start with non-capitalized words.  i is a good variable name.'\n","english_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n","english_tokenizer.tokenize(etext)\n","\n"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Punkt knows that the periods in Mr. Smith and Johann S. Bach do not mark sentence boundaries.',\n"," 'And sometimes sentences can start with non-capitalized words.',\n"," 'i is a good variable name.']"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZhiMSsUskKEq","executionInfo":{"status":"ok","timestamp":1634617070382,"user_tz":-420,"elapsed":346,"user":{"displayName":"พีรเดช สาตสําอางค์","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjerzRPWnypjYRSvGre-9VnuCETGoQsAxXwMDSZA=s64","userId":"05999278997587725662"}},"outputId":"e82295b8-94d1-4eb9-f38d-b6439be04ef5"},"source":["#ภาษาเยอรมัน\n","german_tokenizer = nltk.data.load('tokenizers/punkt/PY3/german.pickle')  \n","gtext = 'Wie geht es Ihnen? Mir geht es gut.'\n","german_tokenizer.tokenize(gtext) "],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Wie geht es Ihnen?', 'Mir geht es gut.']"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"3b82bmXzkpoO"},"source":["# ภาษาที่มีใน punkt\n","\n","czech.pickle     finnish.pickle  norwegian.pickle   slovene.pickle\n","danish.pickle    french.pickle   polish.pickle      spanish.pickle\n","dutch.pickle     german.pickle   portuguese.pickle  swedish.pickle\n","english.pickle   greek.pickle                turkish.pickle\n","estonian.pickle  italian.pickle"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qx1V2YutkPt-","executionInfo":{"status":"ok","timestamp":1634617075870,"user_tz":-420,"elapsed":534,"user":{"displayName":"พีรเดช สาตสําอางค์","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjerzRPWnypjYRSvGre-9VnuCETGoQsAxXwMDSZA=s64","userId":"05999278997587725662"}},"outputId":"c9a99dc4-fcaa-4ce3-bc07-992f235f168a"},"source":["#ภาษาเยอรมัน\n","german_tokenizer = nltk.data.load('tokenizers/punkt/PY3/german.pickle')  \n","gtext = 'Wie geht es Ihnen? Mir geht es gut.'\n","german_tokenizer.tokenize(gtext) "],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Wie geht es Ihnen?', 'Mir geht es gut.']"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"eMG9CbcBk3JH"},"source":["# ตัดคำ (word_tokenize)"]},{"cell_type":"code","metadata":{"id":"AMAa2dymH7f_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634617078359,"user_tz":-420,"elapsed":4,"user":{"displayName":"พีรเดช สาตสําอางค์","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjerzRPWnypjYRSvGre-9VnuCETGoQsAxXwMDSZA=s64","userId":"05999278997587725662"}},"outputId":"6b31edde-2ac4-45ed-87fd-7c77510bfe9f"},"source":["#Tokenization --Text into word tokens;\n","from nltk.tokenize import word_tokenize \n","  \n","text = \"Hello All. Welcome to medium. This article is about NLP using NLTK. Subscribe with $4.00. \"\n","print(\"SENTENCE AS TOKENS:\")\n","print(word_tokenize(text))\n","print(\"No of Sentence Tokens:\",len(word_tokenize(text)))\n"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["SENTENCE AS TOKENS:\n","['Hello', 'All', '.', 'Welcome', 'to', 'medium', '.', 'This', 'article', 'is', 'about', 'NLP', 'using', 'NLTK', '.', 'Subscribe', 'with', '$', '4.00', '.']\n","No of Sentence Tokens: 20\n"]}]},{"cell_type":"code","metadata":{"id":"cdYqZcYCINaK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634617116374,"user_tz":-420,"elapsed":302,"user":{"displayName":"พีรเดช สาตสําอางค์","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjerzRPWnypjYRSvGre-9VnuCETGoQsAxXwMDSZA=s64","userId":"05999278997587725662"}},"outputId":"c0f4ad33-a4da-4b93-ae83-84bd1cc84bb8"},"source":["#Treebank หรือ คลังต้นไม้ คือ คลังข้อความที่ในแต่ละประโยคได้กำกับโครงสร้างวากยสัมพันธ์ สังเกตผลลัพธ์ที่ได้จากการทำ wordtokenizer กับ treebank\n","from nltk.tokenize import TreebankWordTokenizer \n","text = \"Hello All. Welcome to medium. This article is about NLP using NLTK. Subscribe with $4.00. \"\n","tokenizer = TreebankWordTokenizer() \n","tokenizer.tokenize(text) "],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hello',\n"," 'All.',\n"," 'Welcome',\n"," 'to',\n"," 'medium.',\n"," 'This',\n"," 'article',\n"," 'is',\n"," 'about',\n"," 'NLP',\n"," 'using',\n"," 'NLTK.',\n"," 'Subscribe',\n"," 'with',\n"," '$',\n"," '4.00',\n"," '.']"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"5yFvaxNeTc-r"},"source":["###n-grams vs tokens\n","\n","##### n-grams are contiguous sequences of n-items in a sentence. N can be 1, 2 or any other positive integers, although usually we do not consider very large N because those n-grams rarely appears in many different places.\n","\n","##### Tokens do not have any conditions on contiguity"]},{"cell_type":"markdown","metadata":{"id":"8jucigmFmapg"},"source":["# Regular expression (re) \n","ใน python. ... regex เป็นรูปแบบการเขียนที่นิยมใช้กันทั่วไปในการแสดงรูปแบบของตัวหนังสือ หากใช้ regex แล้วจะทำให้สามารถค้นหากลุ่มตัวหนังสือที่มีรูปแบบตามที่ต้องการจากข้อความหรือกลุ่มตัวอักษรได้ \n","\n","ดูเพิ่มเติมที่ https://www.bualabs.com/archives/3070/what-is-regular-expression-regex-regexp-teach-how-to-regex-python-nlp-ep-7/"]},{"cell_type":"code","metadata":{"id":"x_3jEcxcVbA-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634617119816,"user_tz":-420,"elapsed":334,"user":{"displayName":"พีรเดช สาตสําอางค์","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjerzRPWnypjYRSvGre-9VnuCETGoQsAxXwMDSZA=s64","userId":"05999278997587725662"}},"outputId":"9b3b069b-b773-4ae2-be09-9bc68cbe72f4"},"source":["#Using pure python\n","import re \n","def generate_ngrams(text, n):\n","    # Convert to lowercases\n","    text = text.lower()\n","    \n","    # Replace all none alphanumeric characters with spaces\n","    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n","    \n","    # Break sentence in the token, remove empty tokens\n","    tokens = [token for token in text.split(\" \") if token != \"\"]\n","    \n","    # Use the zip function to help us generate n-grams\n","    # Concatentate the tokens into ngrams and return\n","    ngrams = zip(*[tokens[i:] for i in range(n)])\n","    return [\" \".join(ngram) for ngram in ngrams]\n","\n","text = \"Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\"\n","print(text)\n","generate_ngrams(text, n=3)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\n"]},{"output_type":"execute_result","data":{"text/plain":["['hello everyone welcome',\n"," 'everyone welcome to',\n"," 'welcome to intro',\n"," 'to intro to',\n"," 'intro to machine',\n"," 'to machine learning',\n"," 'machine learning applications',\n"," 'learning applications we',\n"," 'applications we are',\n"," 'we are now',\n"," 'are now learning',\n"," 'now learning important',\n"," 'learning important basics',\n"," 'important basics of',\n"," 'basics of nlp']"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"wz-Mq1T6YQSW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634617124050,"user_tz":-420,"elapsed":351,"user":{"displayName":"พีรเดช สาตสําอางค์","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjerzRPWnypjYRSvGre-9VnuCETGoQsAxXwMDSZA=s64","userId":"05999278997587725662"}},"outputId":"249b407c-46a8-4a32-8cc0-59f6fc1637f6"},"source":["#Using NLTK import ngrams\n","\n","import re\n","from nltk.util import ngrams\n","\n","text = text.lower()\n","text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n","tokens = [token for token in text.split(\" \") if token != \"\"]\n","output = list(ngrams(tokens, 3))\n","print(output)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[('hello', 'everyone', 'welcome'), ('everyone', 'welcome', 'to'), ('welcome', 'to', 'intro'), ('to', 'intro', 'to'), ('intro', 'to', 'machine'), ('to', 'machine', 'learning'), ('machine', 'learning', 'applications'), ('learning', 'applications', 'we'), ('applications', 'we', 'are'), ('we', 'are', 'now'), ('are', 'now', 'learning'), ('now', 'learning', 'important'), ('learning', 'important', 'basics'), ('important', 'basics', 'of'), ('basics', 'of', 'nlp')]\n"]}]},{"cell_type":"code","metadata":{"id":"9BG909xTFbeZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634617127198,"user_tz":-420,"elapsed":872,"user":{"displayName":"พีรเดช สาตสําอางค์","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjerzRPWnypjYRSvGre-9VnuCETGoQsAxXwMDSZA=s64","userId":"05999278997587725662"}},"outputId":"9b7eed06-f496-454e-9e4a-26826c3b82fb"},"source":["#Text Normalization\n","\n","#Case Conversion\n","text = \"Hello All. Welcome to medium. This article is about NLP using NLTK. Subscribe with $4.00.\"\n","lowert = text.lower()\n","uppert = text.upper()\n","\n","print(\"To Lower Case:\",lowert)\n","print(\"To Upper Case:\",uppert)\n"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["To Lower Case: hello all. welcome to medium. this article is about nlp using nltk. subscribe with $4.00.\n","To Upper Case: HELLO ALL. WELCOME TO MEDIUM. THIS ARTICLE IS ABOUT NLP USING NLTK. SUBSCRIBE WITH $4.00.\n"]}]},{"cell_type":"markdown","metadata":{"id":"XQ0aHZ8HpmO4"},"source":["# #stemming\n","ดูรายละเอียด https://www.bualabs.com/archives/2952/what-is-stemming-what-is-lemmatization-different-stemming-lemmatization-nlp-ep-3/\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dl3U13TSaAdW","executionInfo":{"status":"ok","timestamp":1634617130638,"user_tz":-420,"elapsed":321,"user":{"displayName":"พีรเดช สาตสําอางค์","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjerzRPWnypjYRSvGre-9VnuCETGoQsAxXwMDSZA=s64","userId":"05999278997587725662"}},"outputId":"3fd55221-270e-44bd-8d9f-8bcfdc719017"},"source":["\n","#Porterstemmer is a famous stemming approach\n","from nltk.stem import PorterStemmer \n","from nltk.tokenize import word_tokenize \n","   \n","ps = PorterStemmer()\n","sentence = \"It would be unfair to demand that people cease pirating files when those same people aren't paid for their participation in very lucrative network schemes. Ordinary people are relentlessly spied on, and not compensated for information taken from them. While I'd like to see everyone eventually pay for music and the like, I'd not ask for it until there's reciprocity.\"\n","\n","sent = word_tokenize(sentence)\n","print(\"After Word Tokenization:\\n\",sent)\n","print(\"Total No of Word Tokens: \",len(sent))\n","\n","ps_sent = [ps.stem(words_sent) for words_sent in sent]\n","print(ps_sent)\n","print(len(ps_sent))"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["After Word Tokenization:\n"," ['It', 'would', 'be', 'unfair', 'to', 'demand', 'that', 'people', 'cease', 'pirating', 'files', 'when', 'those', 'same', 'people', 'are', \"n't\", 'paid', 'for', 'their', 'participation', 'in', 'very', 'lucrative', 'network', 'schemes', '.', 'Ordinary', 'people', 'are', 'relentlessly', 'spied', 'on', ',', 'and', 'not', 'compensated', 'for', 'information', 'taken', 'from', 'them', '.', 'While', 'I', \"'d\", 'like', 'to', 'see', 'everyone', 'eventually', 'pay', 'for', 'music', 'and', 'the', 'like', ',', 'I', \"'d\", 'not', 'ask', 'for', 'it', 'until', 'there', \"'s\", 'reciprocity', '.']\n","Total No of Word Tokens:  69\n","['It', 'would', 'be', 'unfair', 'to', 'demand', 'that', 'peopl', 'ceas', 'pirat', 'file', 'when', 'those', 'same', 'peopl', 'are', \"n't\", 'paid', 'for', 'their', 'particip', 'in', 'veri', 'lucr', 'network', 'scheme', '.', 'ordinari', 'peopl', 'are', 'relentlessli', 'spi', 'on', ',', 'and', 'not', 'compens', 'for', 'inform', 'taken', 'from', 'them', '.', 'while', 'I', \"'d\", 'like', 'to', 'see', 'everyon', 'eventu', 'pay', 'for', 'music', 'and', 'the', 'like', ',', 'I', \"'d\", 'not', 'ask', 'for', 'it', 'until', 'there', \"'s\", 'reciproc', '.']\n","69\n"]}]},{"cell_type":"code","metadata":{"id":"1JxdoZyaY-iP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634617133742,"user_tz":-420,"elapsed":315,"user":{"displayName":"พีรเดช สาตสําอางค์","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjerzRPWnypjYRSvGre-9VnuCETGoQsAxXwMDSZA=s64","userId":"05999278997587725662"}},"outputId":"4816b3f3-368f-4567-fa2e-efc8d8541a00"},"source":["#Porter stemmer is a famous stemming approach\n","\n","from nltk.stem import PorterStemmer \n","from nltk.tokenize import word_tokenize \n","ps = PorterStemmer() \n"," \n","words = [\"hike\", \"hikes\", \"hiked\", \"hiking\", \"hikers\", \"hiker\", \"universal\", \"universe\", \"university\",\"alumnus\", \"alumni\", \"alumnae\"] \n","  \n","for w in words: \n","    print(w, \" : \", ps.stem(w)) "],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["hike  :  hike\n","hikes  :  hike\n","hiked  :  hike\n","hiking  :  hike\n","hikers  :  hiker\n","hiker  :  hiker\n","universal  :  univers\n","universe  :  univers\n","university  :  univers\n","alumnus  :  alumnu\n","alumni  :  alumni\n","alumnae  :  alumna\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vwz1NDGUozEY","executionInfo":{"status":"ok","timestamp":1634617146425,"user_tz":-420,"elapsed":307,"user":{"displayName":"พีรเดช สาตสําอางค์","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjerzRPWnypjYRSvGre-9VnuCETGoQsAxXwMDSZA=s64","userId":"05999278997587725662"}},"outputId":"d5e7a47e-7879-4b0d-a9a0-4587b47d0b1d"},"source":["#another stemmer\n","from nltk.stem.snowball import SnowballStemmer\n","from nltk.tokenize import word_tokenize \n","   \n","sb = SnowballStemmer(\"english\")\n","sentence = \"It would be unfair to demand that people cease pirating files when those same people aren't paid for their participation in very lucrative network schemes. Ordinary people are relentlessly spied on, and not compensated for information taken from them. While I'd like to see everyone eventually pay for music and the like, I'd not ask for it until there's reciprocity.\"\n","\n","sent = word_tokenize(sentence)\n","print(\"After Word Tokenization:\\n\",sent)\n","print(\"Total No of Word Tokens: \",len(sent))\n","\n","sb_sent = [sb.stem(words_sent) for words_sent in sent]\n","print(sb_sent)\n","print(len(sb_sent))"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["After Word Tokenization:\n"," ['It', 'would', 'be', 'unfair', 'to', 'demand', 'that', 'people', 'cease', 'pirating', 'files', 'when', 'those', 'same', 'people', 'are', \"n't\", 'paid', 'for', 'their', 'participation', 'in', 'very', 'lucrative', 'network', 'schemes', '.', 'Ordinary', 'people', 'are', 'relentlessly', 'spied', 'on', ',', 'and', 'not', 'compensated', 'for', 'information', 'taken', 'from', 'them', '.', 'While', 'I', \"'d\", 'like', 'to', 'see', 'everyone', 'eventually', 'pay', 'for', 'music', 'and', 'the', 'like', ',', 'I', \"'d\", 'not', 'ask', 'for', 'it', 'until', 'there', \"'s\", 'reciprocity', '.']\n","Total No of Word Tokens:  69\n","['it', 'would', 'be', 'unfair', 'to', 'demand', 'that', 'peopl', 'ceas', 'pirat', 'file', 'when', 'those', 'same', 'peopl', 'are', \"n't\", 'paid', 'for', 'their', 'particip', 'in', 'veri', 'lucrat', 'network', 'scheme', '.', 'ordinari', 'peopl', 'are', 'relentless', 'spi', 'on', ',', 'and', 'not', 'compens', 'for', 'inform', 'taken', 'from', 'them', '.', 'while', 'i', \"'d\", 'like', 'to', 'see', 'everyon', 'eventu', 'pay', 'for', 'music', 'and', 'the', 'like', ',', 'i', \"'d\", 'not', 'ask', 'for', 'it', 'until', 'there', \"'s\", 'reciproc', '.']\n","69\n"]}]},{"cell_type":"markdown","metadata":{"id":"eSg33JckpzDi"},"source":["#WordNet Lemmatization"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L2ihkCM3ktot","executionInfo":{"status":"ok","timestamp":1634617157872,"user_tz":-420,"elapsed":2561,"user":{"displayName":"พีรเดช สาตสําอางค์","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjerzRPWnypjYRSvGre-9VnuCETGoQsAxXwMDSZA=s64","userId":"05999278997587725662"}},"outputId":"3c96c87c-b7e1-4203-8b65-2cf26edea8e5"},"source":["#WordNet Lemmatization\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","\n","#without POS tagging\n","text = \"She jumped into the river and breathed heavily\"\n","wordnet = WordNetLemmatizer()\n","tokenizer = word_tokenize(text)\n","\n","for token in tokenizer:\n","    print(token,\"--->\",wordnet.lemmatize(token))"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","She ---> She\n","jumped ---> jumped\n","into ---> into\n","the ---> the\n","river ---> river\n","and ---> and\n","breathed ---> breathed\n","heavily ---> heavily\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X_MDgYUysU3A","outputId":"3577b4bd-5f7d-44b5-84dd-7d9bd2b6a629"},"source":["#Lemmatizer with POS tag\n","from nltk import word_tokenize,pos_tag\n","\n","for token,tag in pos_tag(word_tokenize(text)):\n","    pos=tag[0].lower()\n","        \n","    if pos not in ['a', 'r', 'n', 'v']:\n","        pos='n'\n","    \n","    print(token,\"--->\",wordnet.lemmatize(token,pos))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["She ---> She\n","jumped ---> jump\n","into ---> into\n","the ---> the\n","river ---> river\n","and ---> and\n","breathed ---> breathe\n","heavily ---> heavily\n"]}]},{"cell_type":"markdown","metadata":{"id":"MjfrF4AFs31Y"},"source":["#merge all the tokens to form a long text sequence "]},{"cell_type":"code","metadata":{"id":"x6EM6ADdZYbL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"84d7feaa-a537-447c-c481-ba0a9d2ebe9b"},"source":["#from nltk.stem import PorterStemmer \n","#from nltk.tokenize import word_tokenize \n","#import re\n","   \n","ps = PorterStemmer() \n","text = \"Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\"\n","print(text)\n","\n","\n","#Tokenize and stem the words\n","text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n","tokens = [token for token in text.split(\" \") if token != \"\"]\n","\n","i=0\n","while i<len(tokens):\n","  tokens[i]=ps.stem(tokens[i])\n","  i=i+1\n","\n","#merge all the tokens to form a long text sequence \n","text2 = ' '.join(tokens) \n","\n","print(text2)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\n","hello everyon welcom to intro to machin learn applic We are now learn import basic of nlp\n"]}]},{"cell_type":"markdown","metadata":{"id":"P--sEAHntBeP"},"source":["#stopwords\n","\n","Stop Words คือ คำทั่ว ๆ ไป ที่เราพบบ่อย ๆ ในประโยค หรือ เอกสาร ต่ไม่ค่อยช่วยในการสื่อความหมายสักเท่าไร ทำให้เราสามารถลบคำเหล่านั้นออกไปจากรายการคำศัพท์ได้เลย กรองทิ้งไปจากเอกสารได้เลย เช่น a, an, the, also, just, quite, unless, etc. คำเหล่านี้เรียกว่า Stop Words.  \n","ดูเพิ่มเติมที่ https://colab.research.google.com/github/gnoparus/bualabs/blob/master/nbs/26a_stop_words.ipynb"]},{"cell_type":"code","metadata":{"id":"DQkySHTBldBj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4bff9b85-f85f-4ae2-9148-ed678e1fcbba"},"source":["#Stopwords removal \n","from nltk.corpus import stopwords \n","from nltk.tokenize import word_tokenize \n","\n","text = \"Hello All. Welcome to medium. This article is about NLP using NLTK.\"\n","\n","stop_words = set(stopwords.words('english')) \n","word_tokens = word_tokenize(text) \n","  \n","filtered_sentence = [] \n","  \n","for w in word_tokens: \n","    if w not in stop_words: \n","        filtered_sentence.append(w) \n","  \n","print(word_tokens) \n","print(filtered_sentence) "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', 'All', '.', 'Welcome', 'to', 'medium', '.', 'This', 'article', 'is', 'about', 'NLP', 'using', 'NLTK', '.']\n","['Hello', 'All', '.', 'Welcome', 'medium', '.', 'This', 'article', 'NLP', 'using', 'NLTK', '.']\n"]}]},{"cell_type":"code","metadata":{"id":"ejWwVdZebHlA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"614dc69c-b2f6-448b-8207-fdd98f4e5d5f"},"source":["#Part-of-Speech tagging\n","\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.tag import pos_tag\n","\n","text = \"Medium welcomes you and this article is about NLP using NLTK.\"\n","\n","sent = nltk.word_tokenize(text)\n","print(sent)\n","postag = nltk.pos_tag(sent)\n","print(postag)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Medium', 'welcomes', 'you', 'and', 'this', 'article', 'is', 'about', 'NLP', 'using', 'NLTK', '.']\n","[('Medium', 'NNP'), ('welcomes', 'VBZ'), ('you', 'PRP'), ('and', 'CC'), ('this', 'DT'), ('article', 'NN'), ('is', 'VBZ'), ('about', 'IN'), ('NLP', 'NNP'), ('using', 'VBG'), ('NLTK', 'NNP'), ('.', '.')]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZpDVUGvx5ZGH","outputId":"1aa0a2bc-b439-4b93-8bcb-1fe28e8a8e50"},"source":["nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","\n","example = \"Hello India. Narendra Modi visited China. Daniel Owns a Ford car.\"\n","\n","token = word_tokenize(example)\n","postag = nltk.pos_tag(token)\n","ner = nltk.ne_chunk(postag, binary= False)\n","print(ner)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n","(S\n","  (PERSON Hello/NNP)\n","  (GPE India/NNP)\n","  ./.\n","  (PERSON Narendra/NNP Modi/NNP)\n","  visited/VBD\n","  (GPE China/NNP)\n","  ./.\n","  (PERSON Daniel/NNP Owns/NNP)\n","  a/DT\n","  Ford/NNP\n","  car/NN\n","  ./.)\n"]}]},{"cell_type":"markdown","metadata":{"id":"OFA0LTRgufAT"},"source":["# Named-Entity Recognition\n","เมื่อได้คำนามมาแล้วจากการทำ POS เราจะมาเรียนรู้ Named-Entity Recognition ทำ Named-Entity Tagging ว่าคำ ๆ นี้ เป็น ชื่อสิ่งที่อยู่ในโลกความเป็นจริงหรือไม่ ประเภทอะไร เช่น ชื่อคน สถานที่ องค์กร\n","\n","ดูตัวอย่างเพิ่มเติมที่ https://www.bualabs.com/archives/4112/what-is-part-of-speech-tagging-what-is-named-entity-recognition-tagging-tutorial-pos-tagging-ner-thai-language-pythainlp-ep-4/"]},{"cell_type":"code","metadata":{"id":"8nQvO9BTFdGh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"871c96ba-67b1-49b9-92c5-34bd05a1c9d5"},"source":["#Named entity recognition\n","\n","#spaCy is an NLP Framework -- easy to use and having ability to use neural networks\n","\n","import en_core_web_sm\n","nlp = en_core_web_sm.load()\n","\n","text = 'GitHub is a development platform inspired by the way you work. From open source to business, you can host and review code, manage projects, and build software alongside 40 million developers.'\n","\n","doc = nlp(text)\n","print(doc.ents)\n","print([(X.text, X.label_) for X in doc.ents])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(GitHub, 40 million)\n","[('GitHub', 'ORG'), ('40 million', 'CARDINAL')]\n"]}]},{"cell_type":"markdown","metadata":{"id":"OSi-xIOqu9Hp"},"source":["#Sentiment analysis\n","คือ “การวิเคราะห์ความรู้สึก”\n","เป็นการวิเคราะห์อารมณ์และความรู้สึกจากข้อความ เพื่อบ่งบอกความรู้สึกของผู้คนที่มีต่อบางสิ่งบางอย่าง แบ่งได้เป็น\n","Positive = เป็นในทางที่ดี\n","Negative = เป็นในทางที่ไม่ดี\n","Neutral = เป็นกลาง\n","\n","Example : Get data from twitter\n","https://pypi.org/project/twython/"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k5uODyEUBMC3","outputId":"1114dfcc-c3e4-4377-a13a-ea82af8c2c2c"},"source":["import nltk\n","nltk.download('vader_lexicon')\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","\n","sia = SentimentIntensityAnalyzer()\n","\n","s2 = 'This was the best, most awesome movie EVER MADE!!!'\n","print(\"polarity score for s2:\")\n","sia.polarity_scores(s2)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","polarity score for s2:\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n","  warnings.warn(\"The twython library has not been installed. \"\n"]},{"output_type":"execute_result","data":{"text/plain":["{'compound': 0.8877, 'neg': 0.0, 'neu': 0.425, 'pos': 0.575}"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iIWCdM7WzPnp","executionInfo":{"status":"ok","timestamp":1634617621248,"user_tz":-420,"elapsed":323,"user":{"displayName":"พีรเดช สาตสําอางค์","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjerzRPWnypjYRSvGre-9VnuCETGoQsAxXwMDSZA=s64","userId":"05999278997587725662"}},"outputId":"679f5232-5ffa-465f-d4d8-3b11cc52d546"},"source":["\n","#Porterstemmer is a famous stemming approach\n","from nltk.stem import PorterStemmer \n","from nltk.tokenize import word_tokenize \n","   \n","ps = PorterStemmer()\n","sentence = \"When you get married, you're already fond of what you've seen before, right? So, if you notice a new side and like it after that, it would be double.\"\n","\n","sent = word_tokenize(sentence)\n","print(\"After Word Tokenization:\\n\",sent)\n","print(\"Total No of Word Tokens: \",len(sent))\n","\n","ps_sent = [ps.stem(words_sent) for words_sent in sent]\n","print(ps_sent)\n","print(len(ps_sent))"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["After Word Tokenization:\n"," ['When', 'you', 'get', 'married', ',', 'you', \"'re\", 'already', 'fond', 'of', 'what', 'you', \"'ve\", 'seen', 'before', ',', 'right', '?', 'So', ',', 'if', 'you', 'notice', 'a', 'new', 'side', 'and', 'like', 'it', 'after', 'that', ',', 'it', 'would', 'be', 'double', '.']\n","Total No of Word Tokens:  37\n","['when', 'you', 'get', 'marri', ',', 'you', \"'re\", 'alreadi', 'fond', 'of', 'what', 'you', \"'ve\", 'seen', 'befor', ',', 'right', '?', 'So', ',', 'if', 'you', 'notic', 'a', 'new', 'side', 'and', 'like', 'it', 'after', 'that', ',', 'it', 'would', 'be', 'doubl', '.']\n","37\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gPHsRroxzr9m","executionInfo":{"status":"ok","timestamp":1634617731087,"user_tz":-420,"elapsed":477,"user":{"displayName":"พีรเดช สาตสําอางค์","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjerzRPWnypjYRSvGre-9VnuCETGoQsAxXwMDSZA=s64","userId":"05999278997587725662"}},"outputId":"31ba916d-b825-40cc-9974-9e7cd0ff574d"},"source":["#another stemmer\n","from nltk.stem.snowball import SnowballStemmer\n","from nltk.tokenize import word_tokenize \n","   \n","sb = SnowballStemmer(\"english\")\n","sentence = \"When you get married, you're already fond of what you've seen before, right? So, if you notice a new side and like it after that, it would be double.\"\n","\n","sent = word_tokenize(sentence)\n","print(\"After Word Tokenization:\\n\",sent)\n","print(\"Total No of Word Tokens: \",len(sent))\n","\n","sb_sent = [sb.stem(words_sent) for words_sent in sent]\n","print(sb_sent)\n","print(len(sb_sent))"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["After Word Tokenization:\n"," ['When', 'you', 'get', 'married', ',', 'you', \"'re\", 'already', 'fond', 'of', 'what', 'you', \"'ve\", 'seen', 'before', ',', 'right', '?', 'So', ',', 'if', 'you', 'notice', 'a', 'new', 'side', 'and', 'like', 'it', 'after', 'that', ',', 'it', 'would', 'be', 'double', '.']\n","Total No of Word Tokens:  37\n","['when', 'you', 'get', 'marri', ',', 'you', 're', 'alreadi', 'fond', 'of', 'what', 'you', 've', 'seen', 'befor', ',', 'right', '?', 'so', ',', 'if', 'you', 'notic', 'a', 'new', 'side', 'and', 'like', 'it', 'after', 'that', ',', 'it', 'would', 'be', 'doubl', '.']\n","37\n"]}]}]}