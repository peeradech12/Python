from nltk.tokenize import regexp_tokenize
from nltk.tokenize import word_tokenize

german_text = "Wann gehen wir Pizza essen? ğŸ•ğŸ• Und fÃ¤hrst du mit Ãœber? ğŸš•ğŸš•" 

# Tokenize and print all words in german_text
#all_words = word_tokenize(german_text)
#print(all_words)

#capital_words = r"[A-ZÃœ]\w+"
#print(regexp_tokenize(german_text, capital_words))
emoji = "['\U0001F300-\U0001F5FF'|'\U0001F600-\U0001F64F'|'\U0001F680-\U0001F6FF'|'\u2600-\u26FF\u2700-\u27BF']"
print(regexp_tokenize(german_text, emoji))
